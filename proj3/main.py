# shbang
# Judah Tanninen
# Project 3 Trees vs. Lines
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor

class SelectColumns(BaseEstimator, TransformerMixin): 
    def __init__(self, columns):
        self.columns = columns    
    def fit(self, xs, ys, **params):
        return self    
    def transform(self, xs):
        return xs[self.columns]
    
# Returns a dict with old columns as the keys, and the values are the array of new column names generated by get dummies
def getNewColumns(columns, df):
    new = []
    for col in columns:
        # Find the new columns related to the old column
        related_columns = [new_col for new_col in df.columns if new_col.startswith(col)]
        # Store the mapping in the dictionary
        new += related_columns
    return new

# Read the csv into a dataframe
df = pd.read_csv('AmesHousing.csv')

# Numerical columns we will use (picked randomly by me)
num_features = ['Lot Area', 'Lot Frontage', 'Overall Qual', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Garage Area', 'Garage Cars', 'TotRms AbvGrd', 'Fireplaces', 'Gr Liv Area']
# Categorical features we will use. (also picked randomly)
cat_features=[ 'House Style', 'Bldg Type', 'Kitchen Qual', 'Garage Qual', 'Paved Drive', 'Fence', 'Functional', 'Bsmt Qual', 'Exter Qual', 'Garage Type']

catDF = pd.get_dummies(df[cat_features], dtype=float)
df = pd.concat([df, catDF], axis=1)
df = df.fillna(0)


# Create relations between old categorical features and the new dummy columns, so we can easily use the new columns
catFeatures = getNewColumns(cat_features, catDF)

ys = df['SalePrice']
xs = df.drop('SalePrice', axis=1)

# Add the new columns to the dataframe
xs = pd.concat([xs, catDF], axis=1)
# All columns
columns = [num_features + catFeatures]


# Create an array of all the models

models = [
    {
        'name': 'Linear Regression',
        'model': LinearRegression(), 
        'params': {
            'column_select__columns': columns,
            'model__n_jobs': [-1],
        },
    },
    {
        'name': 'Random Forest',
        'model': RandomForestRegressor(),
        'params': {
            'column_select__columns': columns,
            'model__n_jobs': [-1],
            'model__max_depth': [None, 2, 3, 5],
            'model__max_features': ['sqrt', 'log2', None],
        },
    },
    {
        'name': 'Decision Tree',
        'model': DecisionTreeRegressor(),
        'params': {
            'column_select__columns': columns,
            'model__max_depth': [None, 2, 3, 5],
            'model__max_features': ['sqrt', 'log2', None],
        },
    },
    {
        'name': 'Gradient Boosting',
        'model': GradientBoostingRegressor(),
        'params': {
            'column_select__columns': columns,
            'model__n_estimators': [100, 200],
            'model__max_depth': [None, 2, 3, 5],
            'model__max_features': [None, 'sqrt', 'log2'],
        },
    },
]

# Loop through the models and params in the grade and run the model

for item in models:
    model = item['model'] # The actual function to run
    params = item['params'] # Params to give to the function
    name = item['name']
    # Create the pipeline
    pipe = Pipeline([
        ('column_select', SelectColumns([])),
        ('model', model),
    ])

    # Make the grid search
    search = GridSearchCV(pipe, params, scoring='r2', n_jobs=-1)
    search.fit(xs, ys)

    # Print the scores and params.
    print(name + ":")
    print(search.best_score_)
    print(search.best_params_)

